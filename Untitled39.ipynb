{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0549eaa8-0d86-4f3b-a127-e10fb07cea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/punyatomar/miniconda3/envs/tensorflow/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3527.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Perform panoptic segmentation\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m panoptic_seg, segments_info \u001b[38;5;241m=\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpanoptic_seg\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Visualize the result\u001b[39;00m\n\u001b[1;32m     30\u001b[0m visualizer \u001b[38;5;241m=\u001b[39m Visualizer(frame[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], MetadataCatalog\u001b[38;5;241m.\u001b[39mget(predictor\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mDATASETS\u001b[38;5;241m.\u001b[39mTEST[\u001b[38;5;241m0\u001b[39m]), scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n",
      "File \u001b[0;32m~/detectron2/detectron2/engine/defaults.py:312\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# whether the model expects BGR inputs or RGB\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     original_image \u001b[38;5;241m=\u001b[39m original_image[:, :, ::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 312\u001b[0m height, width \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    313\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug\u001b[38;5;241m.\u001b[39mget_transform(original_image)\u001b[38;5;241m.\u001b[39mapply_image(original_image)\n\u001b[1;32m    314\u001b[0m image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(image\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# Set up Detectron2 configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set threshold for segmentation masks\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
    "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create a predictor\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "# Set up webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Perform panoptic segmentation\n",
    "    panoptic_seg, segments_info = predictor(frame)[\"panoptic_seg\"]\n",
    "\n",
    "    # Visualize the result\n",
    "    visualizer = Visualizer(frame[:, :, ::-1], MetadataCatalog.get(predictor.cfg.DATASETS.TEST[0]), scale=1.2)\n",
    "    vis_frame = visualizer.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow(\"Webcam Panoptic Segmentation\", vis_frame.get_image()[:, :, ::-1])\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
